{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# PCA 적용 함수 정의\n",
    "def apply_pca(X_train, X_test, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train.reshape(X_train.shape[0], -1))  # 차원 축소\n",
    "    X_test_pca = pca.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    \n",
    "    # RNN 모델의 입력 형태로 다시 변환 (배치 크기, 시퀀스 길이, 차원)\n",
    "    X_train_pca = X_train_pca.reshape(X_train_pca.shape[0], -1, n_components)\n",
    "    X_test_pca = X_test_pca.reshape(X_test_pca.shape[0], -1, n_components)\n",
    "    \n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "# 데이터 불러오기 및 전처리 (생략된 부분은 기존 코드 참조)\n",
    "data = pd.read_csv('merged_data_final2.csv')  # 예시 파일 이름\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data['Timestamp'] = data['Timestamp'].astype('int64') // 10**9\n",
    "data = data[['Timestamp', 'DI_uiSpeed', 'DI_vehicleSpeed','Physical_value','acc_value']]\n",
    "\n",
    "# 기존 전처리 과정 그대로\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# 시퀀스 생성 (150 시퀀스 길이로 설정)\n",
    "sequence_length = 150\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(data_normalized) - sequence_length):\n",
    "    X.append(data_normalized[i:i+sequence_length, :-1])\n",
    "    y.append(data_normalized[i+sequence_length, -1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PCA 적용 (축소된 차원 수 설정: 예시로 2)\n",
    "n_components = 2\n",
    "X_train_pca, X_test_pca = apply_pca(X_train, X_test, n_components)\n",
    "\n",
    "# Tensor 변환\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_normalized.shape: (3192, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 데이터 불러오기 및 가공\n",
    "data = pd.read_csv('test_data.csv')\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data['Timestamp'] = data['Timestamp'].astype('int64') // 10**9\n",
    "data = data[['Timestamp', 'uiSpeed', 'vehicleSpeed', 'acc_value','Physical_value']]\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "print(f\"data_normalized.shape: {data_normalized.shape}\")\n",
    "\n",
    "# 시퀀스 생성\n",
    "sequence_length = 150  # 시퀀스 길이\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# acc_value를 타겟으로 설정 (data_normalized의 마지막 열)\n",
    "for i in range(len(data_normalized) - sequence_length):\n",
    "    X.append(data_normalized[i:i+sequence_length, :-1])\n",
    "    y.append(data_normalized[i+sequence_length, -1])  # acc_value를 타겟으로 사용\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PCA 적용 (축소된 차원 수 설정: 예시로 2)\n",
    "n_components = 2\n",
    "X_train_pca, X_test_pca = apply_pca(X_train, X_test, n_components)\n",
    "\n",
    "# Tensor 변환\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 타임스텝의 출력을 가져옴\n",
    "        out = self.sigmoid(out)  # Binary classification을 위해 Sigmoid 적용\n",
    "        return out\n",
    "\n",
    "input_size = n_components  # PCA로 축소한 차원을 사용\n",
    "hidden_size = 64\n",
    "num_layers = 4\n",
    "output_size = 1\n",
    "dropout = 0.5\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Train the model and get the true and predicted labels\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m y_true, y_pred_label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Plot the confusion matrix\u001b[39;00m\n\u001b[0;32m    106\u001b[0m plot_confusion_matrix(y_true, y_pred_label)\n",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#train_time = time.time() - start_time\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m test_loss, test_accuracy, test_precision, test_recall, test_f1, y_true, y_pred_label \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     46\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     47\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[10], line 70\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_dataloader, criterion)\u001b[0m\n\u001b[0;32m     67\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# 예측값과 실제값 저장\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     y_true\u001b[38;5;241m.\u001b[39mextend(y_batch\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     73\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m test_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Confusion Matrix 그리기 함수\n",
    "def plot_confusion_matrix(y_true, y_pred_label):\n",
    "    cm = confusion_matrix(y_true, y_pred_label)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# 학습 및 평가 함수 정의\n",
    "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    global train_losses\n",
    "    train_losses = [] # Train loss 저장 리스트\n",
    "    global test_losses\n",
    "    test_losses = []   # Test loss 저장 리스트\n",
    "    for epoch in range(num_epochs):\n",
    "        #start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        #train_time = time.time() - start_time\n",
    "\n",
    "        test_loss, test_accuracy, test_precision, test_recall, test_f1, y_true, y_pred_label = evaluate_model(model, test_dataloader, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Test Loss: {test_loss:.4f}, '\n",
    "\n",
    "            )\n",
    "        \n",
    "        print(\"-------------------------------------------------------\")\n",
    "    return y_true, y_pred_label  # 최종적으로 y_true와 y_pred_label 반환\n",
    "\n",
    "def evaluate_model(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    global accuracy\n",
    "    global precision\n",
    "    global recall\n",
    "    global f1\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            # 예측값과 실제값 저장\n",
    "            y_pred.extend(outputs.squeeze().cpu().numpy())\n",
    "            y_true.extend(y_batch.squeeze().cpu().numpy())\n",
    "        \n",
    "        test_loss = test_loss / len(test_dataloader.dataset)\n",
    "        \n",
    "        # 예측값을 0.5 기준으로 이진분류\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred_label = (y_pred >= 0.5).astype(int)\n",
    "        \n",
    "        # 평가 지표 계산\n",
    "        accuracy = accuracy_score(y_true, y_pred_label)\n",
    "        precision = precision_score(y_true, y_pred_label)\n",
    "        recall = recall_score(y_true, y_pred_label)\n",
    "        f1 = f1_score(y_true, y_pred_label)\n",
    "\n",
    "        return test_loss, accuracy, precision, recall, f1, y_true, y_pred_label\n",
    "\n",
    "def plot_losses(train_losses, test_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "num_epochs = 30  # 에포크 수 조정 가능\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Train the model and get the true and predicted labels\n",
    "y_true, y_pred_label = train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred_label)\n",
    "\n",
    "plot_losses(train_losses, test_losses)\n",
    "\n",
    "# 최종 평가 지표 출력\n",
    "evaluate_model(model, test_dataloader, criterion)\n",
    "print(\n",
    "    f'Test Accuracy: {accuracy:.4f}, '\n",
    "    f'Test Precision: {precision:.4f}, '\n",
    "    f'Test Recall: {recall:.4f}, '\n",
    "    f'Test F1-Score: {f1:.4f}, '\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
